{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture\n",
        "\n",
        "1. What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        " - Filters and feature maps are the foundational components of the Convolutional Layer in a Convolutional Neural Network, working together to extract meaningful, hierarchical features from input data, typically images.\n",
        "\n",
        " - Filters are small, learnable, two-dimensional matrices of weights. They act as feature detectors within the CNN.\n",
        "\n",
        "    - Feature Detection: The primary role of a filter is to detect and emphasize a specific, localized visual pattern in the input data.\n",
        "      - In the initial layers of a CNN, different filters learn to detect very low-level features, such as edges, corners, or specific textures.\n",
        "      - In deeper layers, filters learn to combine these low-level features to recognize increasingly complex, abstract patterns, such as object parts or entire objects.\n",
        "    - Convolution Operation: The filter slides across the entire width and height of the input. At each position, it performs an element-wise multiplication between its weights and the corresponding input patch, then sums the results. This single sum becomes one pixel in the output.\n",
        "    - Parameter Sharing: The same filter is applied across the entire input. This not only significantly reduces the number of learnable parameters but also grants the network translational equivariance, meaning it can detect the specific pattern regardless of where it appears in the image.\n",
        "    - Learned Weights: The numerical values within each filter are not hand-designed; they are learned during the network's training process using backpropagation to optimize performance for the specific task.\n",
        "\n",
        "  - A Feature Map is the output generated by one filter after it has been convolved over the entire input.\n",
        "    - Location and Strength of Features: The feature map is a 2D array of activation values. Each value represents the response of the specific filter at a particular spatial location in the input.\n",
        "      - A high value in the feature map indicates that the feature the filter is designed to detect was strongly present in the corresponding region of the input.\n",
        "      - A low value indicates the feature was absent or weak.\n",
        "    - Feature Representation: The feature map is essentially a transformed representation of the input, highlighting where a specific feature exists. If a convolutional layer uses $N$ different filters, it will produce $N$ different feature maps, each highlighting the presence of the pattern learned by its corresponding filter. These maps are often stacked together, forming the 3D output of the convolutional layer.\n",
        "    - Input to Next Layer: The stack of feature maps from one convolutional layer serves as the input for the next layer, allowing the network to build a hierarchy of feature ,combining simple features from early layers into more complex ones in deeper layers.\n",
        "\n",
        "\n",
        "2. Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        " - Padding and Stride are two crucial hyperparameters in the convolutional layer of a CNN that control the spatial arrangement and size of the output feature maps.\n",
        "   1. StrideStride defines the number of steps the filter shifts across the input volume during the convolution operation.\n",
        "   - Concept: If the stride is $S$, the filter moves $S$ pixels horizontally, then $S$ pixels vertically, and repeats.\n",
        "   - Default: The most common stride value is 1.\n",
        "   - Effect on Output: A larger stride causes the filter to skip over input regions, resulting in a smaller output feature map. It also reduces the computational load.\n",
        "   2. Padding involves adding extra rows and columns of values to the border of the input volume before the convolution operation.\n",
        "   - Concept: Padding is used primarily for two reasons:\n",
        "     -  Preserve Spatial Size: It allows you to maintain the spatial dimensions of the input volume after the convolution. Without padding, the output feature map is always smaller than the input.\n",
        "     - Ensure Edge Features: It allows the filter to center itself over the pixels at the boundaries of the input, ensuring that the features at the edges are processed equally, which would otherwise be underrepresented.\n",
        "   - Types:\n",
        "     - \"Valid\" Padding (No Padding): No extra zeros are added. The output size will shrink.\n",
        "     - \"Same\" Padding: Enough zero-padding is added to ensure that the output feature map has the same spatial dimensions as the input feature map. The required padding $P$ is calculated based on the filter size $F$.\n",
        "\n",
        "\n",
        "3. Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "\n",
        " - The receptive field of a neuron in a Convolutional Neural Network is the specific region of the input image that influences the activation of that neuron. It defines the maximum spatial extent of the input that the neuron can \"see\" and use to compute its feature.\n",
        "   - An individual neuron in the first layer has a receptive field equal to the size of the filter.\n",
        "   - As you move deeper into the network, a neuron in a subsequent layer is connected to a patch of the previous layer's feature map. Because that patch itself is connected to a larger area of the original input, the receptive field of the deep neuron grows relative to the original input image.\n",
        "\n",
        " - The controlled growth of the receptive field is arguably the most critical architectural concept that makes deep CNNs effective for complex image tasks.It facilitates the creation of a spatial feature hierarchy:\n",
        "\n",
        "   1. Feature Hierarchy and Context: Deep architectures are designed to extract features at multiple levels of abstraction:\n",
        "      - Shallow Layers: Neurons have small receptive fields, allowing them to detect low-level, localized features like edges, corners, and specific textures.\n",
        "      - Deep Layers: As the receptive field expands, neurons in deeper layers can combine the low-level features into high-level, complex features that represent object parts or even entire objects. This global context is essential for tasks like object recognition and scene understanding.\n",
        "    2. Global Understanding: Many computer vision tasks require understanding the context of the entire image:\n",
        "       - Image Classification: To classify an image as \"cat,\" the final layer must have a receptive field that covers a large portion of the input to confirm the presence of all relevant features.\n",
        "       - Semantic Segmentation: For pixel-wise prediction, each output pixel needs a large receptive field to incorporate surrounding context, preventing the network from making decisions based only on local texture.\n",
        "\n",
        "   3. Parameter Efficiency:\n",
        "      - The CNN achieves this large receptive field by stacking small filters rather than using one massive filter.\n",
        "      - This stacked approach is more parameter-efficient, yet it achieves the same spatial coverage, making deep models practical to train.\n",
        "\n",
        "\n",
        "4. Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.\n",
        "\n",
        " - The number of learnable parameters in a Convolutional Neural Network is a crucial factor influencing model complexity, training time, and memory usage. Both filter size and stride directly influence this number, though they do so in different ways.\n",
        "\n",
        "   1. Influence of Filter Size on Parameters\n",
        "   - The filter size has a direct and linear influence on the number of parameters within a single convolutional layer.\n",
        "   - Calculation: The total number of parameters in a convolutional layer is calculated as: $$\\text{Parameters} = (F \\times F \\times N_{in} + 1) \\times N_{out}$$\n",
        "   - Direct Impact: If you increase the filter size $F$, the $F \\times F$ term in the equation grows quadratically, leading to a significant increase in parameters for that layer.\n",
        "\n",
        "   2. Influence of Stride on ParametersThe stride has no direct influence on the number of learnable parameters in a convolutional layer.\n",
        "   - Parameter Definition: The learnable parameters are the weights of the filters and the bias terms. These are defined by the filter size, the input channels, and the output channels.Stride is a hyperparameter that defines how the convolution operation is performed, but it does not change the internal definition of the filter itself.\n",
        "   - Indirect Impact: While stride doesn't change the parameters of the current layer, it has a profound indirect influence on the parameter count of subsequent layers:\n",
        "      - A larger stride reduces the spatial dimensions of the current layer's output feature map.\n",
        "      - If the next layer is a fully connected layer, the total number of connections in that FC layer is proportional to the size of the feature map it receives. A smaller feature map drastically reduces the parameter count in the subsequent FC layer.\n",
        "      - If the next layer is another convolutional layer, a smaller feature map size reduces the memory and computation required for that subsequent convolution, but the number of parameters in the subsequent convolution itself remains determined only by its own filter size and channel counts.\n",
        "\n",
        "\n",
        "5. Compare and contrast different CNN-based architectures like LeNet,AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        " - LeNet\n",
        "    - Depth / structure: Shallow: ~5-7 layers (Conv → Pool → Conv → Pool → FC).\n",
        "    - Filter sizes: Typically 5*5 in conv layers.\n",
        "    - Dataset / use: Designed for small grayscale images.\n",
        "    - Activation / normalization: Originally used sigmoid/tanh; no batch norm.\n",
        "    - Performance: Good for digit recognition; too small for large-scale images.\n",
        "    - Notes: Pioneering architecture that introduced conv + subsampling and end-to-end training.\n",
        "\n",
        " - AlexNet\n",
        "    - Depth: ~8 learned layers. Much deeper than LeNet.\n",
        "    - Filter sizes: Early layer used large 11*11 filters followed by 5*5, 33.\n",
        "    - Dataset: ImageNet — demonstrated large-scale viability.\n",
        "    - Activation / normalization: ReLU activations, dropout, data augmentation; used GPUs and local response norm.\n",
        "    - Performance: Big leap in ImageNet accuracy.\n",
        "    - Notes: Introduced practices still used today: ReLU, aggressive augmentation, dropout, GPU training.\n",
        "\n",
        " - VGG\n",
        "    - Depth: Very deep — common variants: VGG16, VGG19.\n",
        "    - Filter sizes: Uniform use of 3*3 conv filters.\n",
        "    - Design philosophy: Replace large filters with multiple 3*3 to get more nonlinearity and fewer params per receptive field.\n",
        "    - Performance: Strong ImageNet performance; descriptors useful for transfer learning.\n",
        "    - Cost: High parameter count and compute.\n",
        "    - Notes: Simplicity and uniformity made it extremely popular for feature extraction.\n",
        "\n",
        "6. Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "- Answer\n",
        "      \n",
        "      import tensorflow as tf\n",
        "      from tensorflow.keras import layers, models, datasets\n",
        "\n",
        "      # 1 Load + preprocess\n",
        "      (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "      x_train = x_train.reshape(-1,28,28,1).astype(\"float32\")/255.0\n",
        "      x_test  = x_test.reshape(-1,28,28,1).astype(\"float32\")/255.0\n",
        "\n",
        "       # 2 Model creation\n",
        "       model = models.Sequential([\n",
        "          layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "          layers.BatchNormalization(),\n",
        "          layers.MaxPooling2D((2,2)),\n",
        "          layers.Conv2D(64, (3,3), activation='relu'),\n",
        "          layers.BatchNormalization(),\n",
        "          layers.MaxPooling2D((2,2)),\n",
        "          layers.Flatten(),\n",
        "          layers.Dense(128, activation='relu'),\n",
        "          layers.Dropout(0.4),\n",
        "          layers.Dense(10, activation='softmax')\n",
        "       ])\n",
        "\n",
        "      # 3 Compile\n",
        "      model.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "      model.summary()\n",
        "\n",
        "      # 4 Train\n",
        "      history = model.fit(x_train, y_train, epochs=6, batch_size=128, validation_split=0.1)\n",
        "\n",
        "      # 5 Evaluate\n",
        "      test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "      print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "7.  Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        " - Answer\n",
        "\n",
        "       import tensorflow as tf\n",
        "       from tensorflow.keras import layers, models, datasets\n",
        "       from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "       # 1) Load\n",
        "       (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "       x_train = x_train.astype(\"float32\")/255.0\n",
        "       x_test  = x_test.astype(\"float32\")/255.0\n",
        "\n",
        "       # 2) Data augmentation\n",
        "       datagen = ImageDataGenerator(\n",
        "           rotation_range=15,\n",
        "           width_shift_range=0.1,\n",
        "           height_shift_range=0.1,\n",
        "           horizontal_flip=True,\n",
        "           validation_split=0.1\n",
        "       )\n",
        "       train_gen = datagen.flow(x_train, y_train, batch_size=64, subset='training')\n",
        "       val_gen   = datagen.flow(x_train, y_train, batch_size=64, subset='validation')\n",
        "\n",
        "       # 3) Model architecture (simple but effective)\n",
        "       model = models.Sequential([\n",
        "           layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)),\n",
        "           layers.BatchNormalization(),\n",
        "           layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
        "           layers.BatchNormalization(),\n",
        "           layers.MaxPooling2D((2,2)),\n",
        "           layers.Dropout(0.25),\n",
        "\n",
        "           layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "           layers.BatchNormalization(),\n",
        "           layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "           layers.BatchNormalization(),\n",
        "           layers.MaxPooling2D((2,2)),\n",
        "           layers.Dropout(0.25),\n",
        "\n",
        "           layers.Flatten(),\n",
        "           layers.Dense(512, activation='relu'),\n",
        "           layers.BatchNormalization(),\n",
        "           layers.Dropout(0.5),\n",
        "           layers.Dense(10, activation='softmax')\n",
        "       ])\n",
        "\n",
        "       model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "       model.summary()\n",
        "\n",
        "       # 4) Train\n",
        "       model.fit(train_gen, epochs=30, validation_data=val_gen)\n",
        "\n",
        "       # 5) Evaluate\n",
        "       print(model.evaluate(x_test, y_test))\n",
        "\n",
        "8.  Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "- Answer\n",
        "           \n",
        "      import torch\n",
        "      import torch.nn as nn\n",
        "      import torch.optim as optim\n",
        "      from torchvision import datasets, transforms\n",
        "      from torch.utils.data import DataLoader\n",
        "\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "      # 1) Data loaders\n",
        "      transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      train_ds = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "      test_ds  = datasets.MNIST('data', train=False, transform=transform)\n",
        "\n",
        "      train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "      test_loader  = DataLoader(test_ds, batch_size=1000)\n",
        "\n",
        "      # 2) Model\n",
        "      class SimpleCNN(nn.Module):\n",
        "          def __init__(self):\n",
        "              super().__init__()\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.Conv2d(1, 32, 3, 1), nn.ReLU(),\n",
        "                  nn.Conv2d(32, 64, 3, 1), nn.ReLU(),\n",
        "                  nn.MaxPool2d(2),\n",
        "                  nn.Dropout(0.25)\n",
        "              )\n",
        "              self.fc = nn.Sequential(\n",
        "                  nn.Flatten(),\n",
        "                  nn.Linear(64*12*12, 128),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Dropout(0.5),\n",
        "                  nn.Linear(128, 10)\n",
        "              )\n",
        "\n",
        "          def forward(self, x):\n",
        "              x = self.conv(x)\n",
        "              return self.fc(x)\n",
        "      \n",
        "      model = SimpleCNN().to(device)\n",
        "      optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      \n",
        "      # 3) Training loop\n",
        "      def train(epoch):\n",
        "          model.train()\n",
        "          total_loss = 0\n",
        "          for batch_idx, (data, target) in enumerate(train_loader):\n",
        "              data, target = data.to(device), target.to(device)\n",
        "              optimizer.zero_grad()\n",
        "              output = model(data)\n",
        "              loss = criterion(output, target)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              total_loss += loss.item()\n",
        "          print(f\"Epoch {epoch} - Train loss: {total_loss/len(train_loader):.4f\")\n",
        "\n",
        "      def test():\n",
        "          model.eval()\n",
        "          correct = 0\n",
        "          with torch.no_grad():\n",
        "              for data, target in test_loader:\n",
        "                  data, target = data.to(device), target.to(device)\n",
        "                  output = model(data)\n",
        "                  pred = output.argmax(dim=1)\n",
        "                  correct += pred.eq(target).sum().item()\n",
        "          acc = correct / len(test_ds)\n",
        "          print(f\"Test accuracy: {acc:.4f}\")\n",
        "          return acc\n",
        "\n",
        "      for epoch in range(1, 6):\n",
        "          train(epoch)\n",
        "          test()\n",
        "\n",
        "9. Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "- Answer\n",
        "\n",
        "      from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "      from tensorflow.keras import layers, models\n",
        "\n",
        "      # Directory structure:\n",
        "      # dataset/\n",
        "      #   train/\n",
        "      #     classA/\n",
        "      #     classB/\n",
        "      #   val/\n",
        "      #     classA/\n",
        "      #     classB/\n",
        "\n",
        "       train_datagen = ImageDataGenerator(\n",
        "          rescale=1./255,\n",
        "          rotation_range=20,\n",
        "          width_shift_range=0.1,\n",
        "          height_shift_range=0.1,\n",
        "          shear_range=0.1,\n",
        "          zoom_range=0.1,\n",
        "          horizontal_flip=True\n",
        "      )\n",
        "\n",
        "      val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "      train_generator = train_datagen.flow_from_directory(\n",
        "          'dataset/train',\n",
        "          target_size=(150,150),\n",
        "          batch_size=32,\n",
        "          class_mode='categorical'\n",
        "      )\n",
        "\n",
        "       val_generator = val_datagen.flow_from_directory(\n",
        "          'dataset/val',\n",
        "          target_size=(150,150),\n",
        "          batch_size=32,\n",
        "          class_mode='categorical'\n",
        "      )\n",
        "\n",
        "       # Simple CNN\n",
        "       model = models.Sequential([\n",
        "         layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
        "         layers.MaxPooling2D(2,2),\n",
        "         layers.Conv2D(64, (3,3), activation='relu'),\n",
        "         layers.MaxPooling2D(2,2),\n",
        "         layers.Flatten(),\n",
        "         layers.Dense(128, activation='relu'),\n",
        "         layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "      ])\n",
        "     \n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "      # Train\n",
        "      model.fit(train_generator, validation_data=val_generator, epochs=15)\n",
        "\n",
        "      # Save model\n",
        "      model.save('custom_cnn.h5')\n",
        "\n",
        "\n",
        "10.  You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        " - Answer\n",
        "    1. Data & preparation\n",
        "    - Dataset sources: e.g., public chest X-ray datasets.\n",
        "    - Directory structure: data/train/Normal, data/train/Pneumonia, similarly for val and test.\n",
        "    - Preprocessing:\n",
        "      - Convert to RGB, resize.\n",
        "      - Normalize pixel values.\n",
        "      - Apply data augmentation— be conservative with augmentations to avoid unrealistic images.\n",
        "    - Class imbalance: use class weights or oversampling. Keep a held-out test set from a different hospital/source if possible for robustness.\n",
        "\n",
        "    2. Model selection & training\n",
        "    - Recommendation: Use transfer learning— e.g., DenseNet121, ResNet50, EfficientNetB0. DenseNet variants are popular for chest X-ray tasks.\n",
        "    - Architecture skeleton (Keras):\n",
        "\n",
        "          from tensorflow.keras.applications import DenseNet121\n",
        "          from tensorflow.keras import layers, models\n",
        "          from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "          base = DenseNet121(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "          base.trainable = False  # freeze initially\n",
        "\n",
        "          model = models.Sequential([\n",
        "              base,\n",
        "              layers.GlobalAveragePooling2D(),\n",
        "              layers.Dropout(0.5),\n",
        "              layers.Dense(256, activation='relu'),\n",
        "              layers.Dropout(0.3),\n",
        "              layers.Dense(1, activation='sigmoid')  # binary classification\n",
        "          ])\n",
        "\n",
        "          model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
        "\n",
        "    3. Validation & evaluation\n",
        "\n",
        "    - Evaluate on a held-out test set and, if possible, an external dataset from a different hospital.\n",
        "    - Report: ROC curve, AUC, sensitivity, specificity, precision, recall, confusion matrix.\n",
        "    - Use Grad-CAM or other explainability maps to visualize salient regions.\n",
        "\n",
        "    4. Model packaging & saving\n",
        "    - Save model (model.save('xray_model.h5')) and optionally create a lightweight inference wrapper that does preprocessing and returns probability + explanation maps.\n",
        "\n",
        "    5. Streamlit web app (simple inference UI)\n",
        "\n",
        "     # app.py (Streamlit)\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load model (ensure the model file is in the same folder)\n",
        "model = tf.keras.models.load_model('xray_model.h5')\n",
        "\n",
        "st.title(\"Chest X-ray: Normal vs Pneumonia (Demo)\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a chest X-ray image (jpg/png)\", type=['jpg','jpeg','png'])\n",
        "if uploaded_file is not None:\n",
        "    img = Image.open(uploaded_file).convert('RGB').resize((224,224))\n",
        "    st.image(img, caption='Uploaded image', use_column_width=True)\n",
        "    x = np.array(img).astype('float32')/255.0\n",
        "    x = np.expand_dims(x, axis=0)  # batch dimension\n",
        "\n",
        "        prob = model.predict(x)[0][0]\n",
        "         st.write(f\"Model probability (Pneumonia): {prob:.3f}\")\n",
        "    \n",
        "         if prob > 0.5:\n",
        "        st.error(\"Prediction: Pneumonia — probability {:.2f}\".format(prob))\n",
        "         else:\n",
        "            st.success(\"Prediction: Normal — probability {:.2f}\".format(1-prob))\n",
        "\n",
        "          # Optionally: show Grad-CAM heatmap (if implemented)\n"
      ],
      "metadata": {
        "id": "RFFQOkxH-Ptb"
      }
    }
  ]
}